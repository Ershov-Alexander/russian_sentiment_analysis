{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "russian_sentiment_analysis_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuuQJTysxf-l",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "outputId": "952c2951-6b08-48b1-da98-ef4c99e2da5b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers==3.0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "urFUWGenyC_z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from src.training import Trainer, TrainerFactory\n",
    "import pandas as pd\n",
    "from src.prediction import Predictor\n",
    "from src.evaluation import evaluate"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_fOC26QyItH3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "outputId": "5b9feea8-c089-4dcd-f92b-76c57a4bd822"
   },
   "source": [
    "trainer = TrainerFactory.trainer_with_default_model()\n",
    "\n",
    "train = pd.read_csv(\"./data/processed_train.csv\", sep=\"&\")\n",
    "test = pd.read_csv(\"./data/processed_test.csv\", sep=\"&\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-conversational and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CcF5GjowI70I",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "fc17cf4b-376b-43aa-d592-56ec8bb22de0"
   },
   "source": [
    "trainer.train(\n",
    "    epochs=2,\n",
    "    train_df=train,\n",
    "    eval_df=test,\n",
    "    dir_to_save=\"models/model2/\",\n",
    "    max_length=192,\n",
    "    batch_size=32,\n",
    "    report_frequency=5\n",
    ")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Batch: 5 of 727. Loss: 1.15. Time: 0:00:07\n",
      "Batch: 10 of 727. Loss: 1.08. Time: 0:00:17\n",
      "Batch: 15 of 727. Loss: 1.06. Time: 0:00:27\n",
      "Batch: 20 of 727. Loss: 0.96. Time: 0:00:37\n",
      "Batch: 25 of 727. Loss: 0.92. Time: 0:00:46\n",
      "Batch: 30 of 727. Loss: 0.89. Time: 0:00:56\n",
      "Batch: 35 of 727. Loss: 0.83. Time: 0:01:06\n",
      "Batch: 40 of 727. Loss: 0.80. Time: 0:01:15\n",
      "Batch: 45 of 727. Loss: 0.64. Time: 0:01:25\n",
      "Batch: 50 of 727. Loss: 0.82. Time: 0:01:35\n",
      "Batch: 55 of 727. Loss: 0.79. Time: 0:01:44\n",
      "Batch: 60 of 727. Loss: 0.80. Time: 0:01:54\n",
      "Batch: 65 of 727. Loss: 0.63. Time: 0:02:04\n",
      "Batch: 70 of 727. Loss: 0.75. Time: 0:02:13\n",
      "Batch: 75 of 727. Loss: 0.71. Time: 0:02:23\n",
      "Batch: 80 of 727. Loss: 0.78. Time: 0:02:32\n",
      "Batch: 85 of 727. Loss: 0.60. Time: 0:02:42\n",
      "Batch: 90 of 727. Loss: 0.60. Time: 0:02:52\n",
      "Batch: 95 of 727. Loss: 0.59. Time: 0:03:01\n",
      "Batch: 100 of 727. Loss: 0.77. Time: 0:03:11\n",
      "Batch: 105 of 727. Loss: 0.72. Time: 0:03:21\n",
      "Batch: 110 of 727. Loss: 0.77. Time: 0:03:30\n",
      "Batch: 115 of 727. Loss: 0.86. Time: 0:03:40\n",
      "Batch: 120 of 727. Loss: 0.54. Time: 0:03:50\n",
      "Batch: 125 of 727. Loss: 0.70. Time: 0:03:59\n",
      "Batch: 130 of 727. Loss: 0.71. Time: 0:04:09\n",
      "Batch: 135 of 727. Loss: 0.63. Time: 0:04:19\n",
      "Batch: 140 of 727. Loss: 0.78. Time: 0:04:28\n",
      "Batch: 145 of 727. Loss: 0.71. Time: 0:04:38\n",
      "Batch: 150 of 727. Loss: 0.68. Time: 0:04:47\n",
      "Batch: 155 of 727. Loss: 0.64. Time: 0:04:57\n",
      "Batch: 160 of 727. Loss: 0.81. Time: 0:05:07\n",
      "Batch: 165 of 727. Loss: 0.63. Time: 0:05:16\n",
      "Batch: 170 of 727. Loss: 0.74. Time: 0:05:26\n",
      "Batch: 175 of 727. Loss: 0.73. Time: 0:05:36\n",
      "Batch: 180 of 727. Loss: 0.58. Time: 0:05:45\n",
      "Batch: 185 of 727. Loss: 0.61. Time: 0:05:55\n",
      "Batch: 190 of 727. Loss: 0.88. Time: 0:06:05\n",
      "Batch: 195 of 727. Loss: 0.45. Time: 0:06:14\n",
      "Batch: 200 of 727. Loss: 0.45. Time: 0:06:24\n",
      "Batch: 205 of 727. Loss: 0.60. Time: 0:06:34\n",
      "Batch: 210 of 727. Loss: 0.79. Time: 0:06:43\n",
      "Batch: 215 of 727. Loss: 0.63. Time: 0:06:53\n",
      "Batch: 220 of 727. Loss: 0.52. Time: 0:07:03\n",
      "Batch: 225 of 727. Loss: 0.69. Time: 0:07:12\n",
      "Batch: 230 of 727. Loss: 0.69. Time: 0:07:22\n",
      "Batch: 235 of 727. Loss: 1.07. Time: 0:07:32\n",
      "Batch: 240 of 727. Loss: 0.76. Time: 0:07:41\n",
      "Batch: 245 of 727. Loss: 0.83. Time: 0:07:51\n",
      "Batch: 250 of 727. Loss: 0.69. Time: 0:08:00\n",
      "Batch: 255 of 727. Loss: 0.68. Time: 0:08:10\n",
      "Batch: 260 of 727. Loss: 0.68. Time: 0:08:20\n",
      "Batch: 265 of 727. Loss: 0.56. Time: 0:08:29\n",
      "Batch: 270 of 727. Loss: 0.85. Time: 0:08:39\n",
      "Batch: 275 of 727. Loss: 0.58. Time: 0:08:49\n",
      "Batch: 280 of 727. Loss: 0.86. Time: 0:08:58\n",
      "Batch: 285 of 727. Loss: 0.51. Time: 0:09:08\n",
      "Batch: 290 of 727. Loss: 0.88. Time: 0:09:18\n",
      "Batch: 295 of 727. Loss: 0.71. Time: 0:09:27\n",
      "Batch: 300 of 727. Loss: 0.71. Time: 0:09:37\n",
      "Batch: 305 of 727. Loss: 0.73. Time: 0:09:47\n",
      "Batch: 310 of 727. Loss: 0.60. Time: 0:09:56\n",
      "Batch: 315 of 727. Loss: 0.62. Time: 0:10:06\n",
      "Batch: 320 of 727. Loss: 0.60. Time: 0:10:15\n",
      "Batch: 325 of 727. Loss: 0.59. Time: 0:10:25\n",
      "Batch: 330 of 727. Loss: 0.58. Time: 0:10:35\n",
      "Batch: 335 of 727. Loss: 0.51. Time: 0:10:44\n",
      "Batch: 340 of 727. Loss: 0.56. Time: 0:10:54\n",
      "Batch: 345 of 727. Loss: 0.84. Time: 0:11:04\n",
      "Batch: 350 of 727. Loss: 0.71. Time: 0:11:13\n",
      "Batch: 355 of 727. Loss: 0.73. Time: 0:11:23\n",
      "Batch: 360 of 727. Loss: 0.66. Time: 0:11:33\n",
      "Batch: 365 of 727. Loss: 0.79. Time: 0:11:42\n",
      "Batch: 370 of 727. Loss: 0.59. Time: 0:11:52\n",
      "Batch: 375 of 727. Loss: 0.51. Time: 0:12:01\n",
      "Batch: 380 of 727. Loss: 0.75. Time: 0:12:11\n",
      "Batch: 385 of 727. Loss: 0.77. Time: 0:12:21\n",
      "Batch: 390 of 727. Loss: 0.79. Time: 0:12:30\n",
      "Batch: 395 of 727. Loss: 0.44. Time: 0:12:40\n",
      "Batch: 400 of 727. Loss: 0.63. Time: 0:12:50\n",
      "Batch: 405 of 727. Loss: 0.36. Time: 0:12:59\n",
      "Batch: 410 of 727. Loss: 0.77. Time: 0:13:09\n",
      "Batch: 415 of 727. Loss: 0.64. Time: 0:13:19\n",
      "Batch: 420 of 727. Loss: 0.46. Time: 0:13:28\n",
      "Batch: 425 of 727. Loss: 0.58. Time: 0:13:38\n",
      "Batch: 430 of 727. Loss: 0.77. Time: 0:13:48\n",
      "Batch: 435 of 727. Loss: 0.64. Time: 0:13:57\n",
      "Batch: 440 of 727. Loss: 0.62. Time: 0:14:07\n",
      "Batch: 445 of 727. Loss: 0.79. Time: 0:14:17\n",
      "Batch: 450 of 727. Loss: 0.70. Time: 0:14:26\n",
      "Batch: 455 of 727. Loss: 0.76. Time: 0:14:36\n",
      "Batch: 460 of 727. Loss: 0.57. Time: 0:14:45\n",
      "Batch: 465 of 727. Loss: 0.60. Time: 0:14:55\n",
      "Batch: 470 of 727. Loss: 0.60. Time: 0:15:05\n",
      "Batch: 475 of 727. Loss: 0.63. Time: 0:15:14\n",
      "Batch: 480 of 727. Loss: 0.57. Time: 0:15:24\n",
      "Batch: 485 of 727. Loss: 0.79. Time: 0:15:34\n",
      "Batch: 490 of 727. Loss: 0.62. Time: 0:15:43\n",
      "Batch: 495 of 727. Loss: 0.44. Time: 0:15:53\n",
      "Batch: 500 of 727. Loss: 0.76. Time: 0:16:03\n",
      "Batch: 505 of 727. Loss: 0.47. Time: 0:16:12\n",
      "Batch: 510 of 727. Loss: 0.93. Time: 0:16:22\n",
      "Batch: 515 of 727. Loss: 0.37. Time: 0:16:31\n",
      "Batch: 520 of 727. Loss: 0.59. Time: 0:16:41\n",
      "Batch: 525 of 727. Loss: 0.49. Time: 0:16:51\n",
      "Batch: 530 of 727. Loss: 0.80. Time: 0:17:00\n",
      "Batch: 535 of 727. Loss: 0.53. Time: 0:17:10\n",
      "Batch: 540 of 727. Loss: 0.62. Time: 0:17:20\n",
      "Batch: 545 of 727. Loss: 0.66. Time: 0:17:29\n",
      "Batch: 550 of 727. Loss: 0.53. Time: 0:17:39\n",
      "Batch: 555 of 727. Loss: 0.48. Time: 0:17:49\n",
      "Batch: 560 of 727. Loss: 0.61. Time: 0:17:58\n",
      "Batch: 565 of 727. Loss: 0.54. Time: 0:18:08\n",
      "Batch: 570 of 727. Loss: 0.57. Time: 0:18:18\n",
      "Batch: 575 of 727. Loss: 0.67. Time: 0:18:27\n",
      "Batch: 580 of 727. Loss: 0.54. Time: 0:18:37\n",
      "Batch: 585 of 727. Loss: 0.66. Time: 0:18:46\n",
      "Batch: 590 of 727. Loss: 0.73. Time: 0:18:56\n",
      "Batch: 595 of 727. Loss: 0.54. Time: 0:19:06\n",
      "Batch: 600 of 727. Loss: 0.67. Time: 0:19:15\n",
      "Batch: 605 of 727. Loss: 0.60. Time: 0:19:25\n",
      "Batch: 610 of 727. Loss: 0.42. Time: 0:19:35\n",
      "Batch: 615 of 727. Loss: 0.67. Time: 0:19:44\n",
      "Batch: 620 of 727. Loss: 0.61. Time: 0:19:54\n",
      "Batch: 625 of 727. Loss: 0.65. Time: 0:20:04\n",
      "Batch: 630 of 727. Loss: 0.64. Time: 0:20:13\n",
      "Batch: 635 of 727. Loss: 0.39. Time: 0:20:23\n",
      "Batch: 640 of 727. Loss: 0.58. Time: 0:20:32\n",
      "Batch: 645 of 727. Loss: 0.74. Time: 0:20:42\n",
      "Batch: 650 of 727. Loss: 0.71. Time: 0:20:52\n",
      "Batch: 655 of 727. Loss: 0.48. Time: 0:21:01\n",
      "Batch: 660 of 727. Loss: 0.69. Time: 0:21:11\n",
      "Batch: 665 of 727. Loss: 0.67. Time: 0:21:21\n",
      "Batch: 670 of 727. Loss: 0.57. Time: 0:21:30\n",
      "Batch: 675 of 727. Loss: 0.45. Time: 0:21:40\n",
      "Batch: 680 of 727. Loss: 0.38. Time: 0:21:50\n",
      "Batch: 685 of 727. Loss: 0.78. Time: 0:21:59\n",
      "Batch: 690 of 727. Loss: 0.65. Time: 0:22:09\n",
      "Batch: 695 of 727. Loss: 0.60. Time: 0:22:19\n",
      "Batch: 700 of 727. Loss: 0.57. Time: 0:22:28\n",
      "Batch: 705 of 727. Loss: 0.75. Time: 0:22:38\n",
      "Batch: 710 of 727. Loss: 0.51. Time: 0:22:48\n",
      "Batch: 715 of 727. Loss: 0.64. Time: 0:22:57\n",
      "Batch: 720 of 727. Loss: 0.65. Time: 0:23:07\n",
      "Batch: 725 of 727. Loss: 0.41. Time: 0:23:17\n",
      "Average training loss: 0.66\n",
      "Training epoch took: 0:23:22\n",
      "Running Validation...\n",
      "Metrics for eval dataset:\n",
      "Avg precision: 0.75\n",
      "Avg recall: 0.75\n",
      "Avg F1: 0.74\n",
      "Validation Loss: 0.60\n",
      "======== Epoch 2 / 2 ========\n",
      "Batch: 5 of 727. Loss: 0.49. Time: 0:00:07\n",
      "Batch: 10 of 727. Loss: 0.40. Time: 0:00:17\n",
      "Batch: 15 of 727. Loss: 0.47. Time: 0:00:27\n",
      "Batch: 20 of 727. Loss: 0.53. Time: 0:00:36\n",
      "Batch: 25 of 727. Loss: 0.71. Time: 0:00:46\n",
      "Batch: 30 of 727. Loss: 0.30. Time: 0:00:55\n",
      "Batch: 35 of 727. Loss: 0.31. Time: 0:01:05\n",
      "Batch: 40 of 727. Loss: 0.59. Time: 0:01:15\n",
      "Batch: 45 of 727. Loss: 0.56. Time: 0:01:24\n",
      "Batch: 50 of 727. Loss: 0.44. Time: 0:01:34\n",
      "Batch: 55 of 727. Loss: 0.52. Time: 0:01:44\n",
      "Batch: 60 of 727. Loss: 0.43. Time: 0:01:53\n",
      "Batch: 65 of 727. Loss: 0.32. Time: 0:02:03\n",
      "Batch: 70 of 727. Loss: 0.36. Time: 0:02:12\n",
      "Batch: 75 of 727. Loss: 0.64. Time: 0:02:22\n",
      "Batch: 80 of 727. Loss: 0.42. Time: 0:02:32\n",
      "Batch: 85 of 727. Loss: 0.30. Time: 0:02:41\n",
      "Batch: 90 of 727. Loss: 0.39. Time: 0:02:51\n",
      "Batch: 95 of 727. Loss: 0.69. Time: 0:03:00\n",
      "Batch: 100 of 727. Loss: 0.30. Time: 0:03:10\n",
      "Batch: 105 of 727. Loss: 0.45. Time: 0:03:20\n",
      "Batch: 110 of 727. Loss: 0.51. Time: 0:03:29\n",
      "Batch: 115 of 727. Loss: 0.46. Time: 0:03:39\n",
      "Batch: 120 of 727. Loss: 0.50. Time: 0:03:48\n",
      "Batch: 125 of 727. Loss: 0.43. Time: 0:03:58\n",
      "Batch: 130 of 727. Loss: 0.46. Time: 0:04:07\n",
      "Batch: 135 of 727. Loss: 0.27. Time: 0:04:17\n",
      "Batch: 140 of 727. Loss: 0.51. Time: 0:04:27\n",
      "Batch: 145 of 727. Loss: 0.36. Time: 0:04:36\n",
      "Batch: 150 of 727. Loss: 0.55. Time: 0:04:46\n",
      "Batch: 155 of 727. Loss: 0.53. Time: 0:04:56\n",
      "Batch: 160 of 727. Loss: 0.34. Time: 0:05:05\n",
      "Batch: 165 of 727. Loss: 0.36. Time: 0:05:15\n",
      "Batch: 170 of 727. Loss: 0.41. Time: 0:05:24\n",
      "Batch: 175 of 727. Loss: 0.46. Time: 0:05:34\n",
      "Batch: 180 of 727. Loss: 0.46. Time: 0:05:44\n",
      "Batch: 185 of 727. Loss: 0.41. Time: 0:05:53\n",
      "Batch: 190 of 727. Loss: 0.46. Time: 0:06:03\n",
      "Batch: 195 of 727. Loss: 0.75. Time: 0:06:12\n",
      "Batch: 200 of 727. Loss: 0.61. Time: 0:06:22\n",
      "Batch: 205 of 727. Loss: 0.64. Time: 0:06:31\n",
      "Batch: 210 of 727. Loss: 0.39. Time: 0:06:41\n",
      "Batch: 215 of 727. Loss: 0.37. Time: 0:06:51\n",
      "Batch: 220 of 727. Loss: 0.46. Time: 0:07:00\n",
      "Batch: 225 of 727. Loss: 0.33. Time: 0:07:10\n",
      "Batch: 230 of 727. Loss: 0.72. Time: 0:07:19\n",
      "Batch: 235 of 727. Loss: 0.30. Time: 0:07:29\n",
      "Batch: 240 of 727. Loss: 0.45. Time: 0:07:39\n",
      "Batch: 245 of 727. Loss: 0.38. Time: 0:07:48\n",
      "Batch: 250 of 727. Loss: 0.44. Time: 0:07:58\n",
      "Batch: 255 of 727. Loss: 0.31. Time: 0:08:07\n",
      "Batch: 260 of 727. Loss: 0.81. Time: 0:08:17\n",
      "Batch: 265 of 727. Loss: 0.40. Time: 0:08:27\n",
      "Batch: 270 of 727. Loss: 0.49. Time: 0:08:36\n",
      "Batch: 275 of 727. Loss: 0.40. Time: 0:08:46\n",
      "Batch: 280 of 727. Loss: 0.42. Time: 0:08:55\n",
      "Batch: 285 of 727. Loss: 0.67. Time: 0:09:05\n",
      "Batch: 290 of 727. Loss: 0.32. Time: 0:09:15\n",
      "Batch: 295 of 727. Loss: 0.45. Time: 0:09:24\n",
      "Batch: 300 of 727. Loss: 0.30. Time: 0:09:34\n",
      "Batch: 305 of 727. Loss: 0.46. Time: 0:09:43\n",
      "Batch: 310 of 727. Loss: 0.46. Time: 0:09:53\n",
      "Batch: 315 of 727. Loss: 0.29. Time: 0:10:02\n",
      "Batch: 320 of 727. Loss: 0.30. Time: 0:10:12\n",
      "Batch: 325 of 727. Loss: 0.35. Time: 0:10:22\n",
      "Batch: 330 of 727. Loss: 0.32. Time: 0:10:31\n",
      "Batch: 335 of 727. Loss: 0.60. Time: 0:10:41\n",
      "Batch: 340 of 727. Loss: 0.46. Time: 0:10:50\n",
      "Batch: 345 of 727. Loss: 0.48. Time: 0:11:00\n",
      "Batch: 350 of 727. Loss: 0.29. Time: 0:11:10\n",
      "Batch: 355 of 727. Loss: 0.42. Time: 0:11:19\n",
      "Batch: 360 of 727. Loss: 0.48. Time: 0:11:29\n",
      "Batch: 365 of 727. Loss: 0.48. Time: 0:11:38\n",
      "Batch: 370 of 727. Loss: 0.53. Time: 0:11:48\n",
      "Batch: 375 of 727. Loss: 0.31. Time: 0:11:58\n",
      "Batch: 380 of 727. Loss: 0.49. Time: 0:12:07\n",
      "Batch: 385 of 727. Loss: 0.52. Time: 0:12:17\n",
      "Batch: 390 of 727. Loss: 0.32. Time: 0:12:26\n",
      "Batch: 395 of 727. Loss: 0.54. Time: 0:12:36\n",
      "Batch: 400 of 727. Loss: 0.40. Time: 0:12:46\n",
      "Batch: 405 of 727. Loss: 0.49. Time: 0:12:55\n",
      "Batch: 410 of 727. Loss: 0.34. Time: 0:13:05\n",
      "Batch: 415 of 727. Loss: 0.38. Time: 0:13:14\n",
      "Batch: 420 of 727. Loss: 0.38. Time: 0:13:24\n",
      "Batch: 425 of 727. Loss: 0.39. Time: 0:13:34\n",
      "Batch: 430 of 727. Loss: 0.43. Time: 0:13:43\n",
      "Batch: 435 of 727. Loss: 0.34. Time: 0:13:53\n",
      "Batch: 440 of 727. Loss: 0.65. Time: 0:14:02\n",
      "Batch: 445 of 727. Loss: 0.84. Time: 0:14:12\n",
      "Batch: 450 of 727. Loss: 0.32. Time: 0:14:22\n",
      "Batch: 455 of 727. Loss: 0.58. Time: 0:14:31\n",
      "Batch: 460 of 727. Loss: 0.36. Time: 0:14:41\n",
      "Batch: 465 of 727. Loss: 0.50. Time: 0:14:50\n",
      "Batch: 470 of 727. Loss: 0.48. Time: 0:15:00\n",
      "Batch: 475 of 727. Loss: 0.51. Time: 0:15:10\n",
      "Batch: 480 of 727. Loss: 0.34. Time: 0:15:19\n",
      "Batch: 485 of 727. Loss: 0.27. Time: 0:15:29\n",
      "Batch: 490 of 727. Loss: 0.32. Time: 0:15:38\n",
      "Batch: 495 of 727. Loss: 0.40. Time: 0:15:48\n",
      "Batch: 500 of 727. Loss: 0.51. Time: 0:15:58\n",
      "Batch: 505 of 727. Loss: 0.41. Time: 0:16:07\n",
      "Batch: 510 of 727. Loss: 0.37. Time: 0:16:17\n",
      "Batch: 515 of 727. Loss: 0.39. Time: 0:16:26\n",
      "Batch: 520 of 727. Loss: 0.26. Time: 0:16:36\n",
      "Batch: 525 of 727. Loss: 0.21. Time: 0:16:46\n",
      "Batch: 530 of 727. Loss: 0.46. Time: 0:16:55\n",
      "Batch: 535 of 727. Loss: 0.46. Time: 0:17:05\n",
      "Batch: 540 of 727. Loss: 0.55. Time: 0:17:14\n",
      "Batch: 545 of 727. Loss: 0.26. Time: 0:17:24\n",
      "Batch: 550 of 727. Loss: 0.20. Time: 0:17:34\n",
      "Batch: 555 of 727. Loss: 0.62. Time: 0:17:43\n",
      "Batch: 560 of 727. Loss: 0.53. Time: 0:17:53\n",
      "Batch: 565 of 727. Loss: 0.46. Time: 0:18:02\n",
      "Batch: 570 of 727. Loss: 0.33. Time: 0:18:12\n",
      "Batch: 575 of 727. Loss: 0.34. Time: 0:18:22\n",
      "Batch: 580 of 727. Loss: 0.36. Time: 0:18:31\n",
      "Batch: 585 of 727. Loss: 0.33. Time: 0:18:41\n",
      "Batch: 590 of 727. Loss: 0.27. Time: 0:18:50\n",
      "Batch: 595 of 727. Loss: 0.49. Time: 0:19:00\n",
      "Batch: 600 of 727. Loss: 0.28. Time: 0:19:10\n",
      "Batch: 605 of 727. Loss: 0.78. Time: 0:19:19\n",
      "Batch: 610 of 727. Loss: 0.46. Time: 0:19:29\n",
      "Batch: 615 of 727. Loss: 0.56. Time: 0:19:38\n",
      "Batch: 620 of 727. Loss: 0.50. Time: 0:19:48\n",
      "Batch: 625 of 727. Loss: 0.33. Time: 0:19:58\n",
      "Batch: 630 of 727. Loss: 0.41. Time: 0:20:07\n",
      "Batch: 635 of 727. Loss: 0.50. Time: 0:20:17\n",
      "Batch: 640 of 727. Loss: 0.29. Time: 0:20:26\n",
      "Batch: 645 of 727. Loss: 0.32. Time: 0:20:36\n",
      "Batch: 650 of 727. Loss: 0.41. Time: 0:20:46\n",
      "Batch: 655 of 727. Loss: 0.41. Time: 0:20:55\n",
      "Batch: 660 of 727. Loss: 0.40. Time: 0:21:05\n",
      "Batch: 665 of 727. Loss: 0.43. Time: 0:21:14\n",
      "Batch: 670 of 727. Loss: 0.42. Time: 0:21:24\n",
      "Batch: 675 of 727. Loss: 0.41. Time: 0:21:34\n",
      "Batch: 680 of 727. Loss: 0.25. Time: 0:21:43\n",
      "Batch: 685 of 727. Loss: 0.50. Time: 0:21:53\n",
      "Batch: 690 of 727. Loss: 0.39. Time: 0:22:02\n",
      "Batch: 695 of 727. Loss: 0.24. Time: 0:22:12\n",
      "Batch: 700 of 727. Loss: 0.33. Time: 0:22:22\n",
      "Batch: 705 of 727. Loss: 0.53. Time: 0:22:31\n",
      "Batch: 710 of 727. Loss: 0.40. Time: 0:22:41\n",
      "Batch: 715 of 727. Loss: 0.48. Time: 0:22:50\n",
      "Batch: 720 of 727. Loss: 0.42. Time: 0:23:00\n",
      "Batch: 725 of 727. Loss: 0.26. Time: 0:23:10\n",
      "Average training loss: 0.43\n",
      "Training epoch took: 0:23:15\n",
      "Running Validation...\n",
      "Metrics for eval dataset:\n",
      "Avg precision: 0.75\n",
      "Avg recall: 0.76\n",
      "Avg F1: 0.74\n",
      "Validation Loss: 0.62\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wsl9RiR-9Mdb",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "f505af7e-0cf3-4bd3-9616-ef51815df153"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z1UxmpYaO0fL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!cp -r models /content/gdrive/My\\ Drive/russian_sentiment_analysis/"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}